{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "colab": {
      "name": "DL4CV_4_ObjectDet.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matteoalberti/Lectures_AdvCV_Experis2020/blob/main/day4/DL4CV_4_ObjectDet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYVNUiBLNMNP"
      },
      "source": [
        "# Introduction to Object Detection\r\n",
        "\r\n",
        "      This is an extra lecture on Obj Det. Material must be re-write for sharing with ad hoc slides and text. \r\n",
        "\r\n",
        "      Only for AdvanceCV models Lecture : Experis Academy 2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcviVNr_ch7d"
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/real_time_object_detection.jpg\" width=\"500\">\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rzVKCR48QNM"
      },
      "source": [
        "   Email: m.alberti@deeplearningitalia.com\n",
        "\n",
        "   Linkedin:\n",
        "   [linkedin_matteo_alberti](www.linkedin.com/in/matteo-alberti-695570110)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBLBwiI48QNM"
      },
      "source": [
        "# One step at a time : \r\n",
        "\r\n",
        "      Going Backwards!\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbjQ17ErQGQL"
      },
      "source": [
        "## Which will be the core family behind deep learning models?\r\n",
        "\r\n",
        "      We are working still with images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcWYFTTFOLQV"
      },
      "source": [
        "## How should we evaluate?\r\n",
        "\r\n",
        "*Evaluation Metrics: mAP*\r\n",
        "\r\n",
        "**mAP**, short for “mean average precision”.\r\n",
        "- Range : from 0 to 100\r\n",
        "- Higher value is better.\r\n",
        "\r\n",
        "*What is?*\r\n",
        "\r\n",
        "Combine all detections from all test images to draw a precision-recall curve (PR curve) for each class; The “average precision” (AP) is the area under the PR curve. (**Do you remember something?**)\r\n",
        "\r\n",
        "Given that target objects are in different classes, we first compute AP separately for each class, and then average over classes.\r\n",
        "A detection is a true positive if it has “intersection over union” (IoU) with a ground-truth box greater than some threshold (usually 0.5; if so, the metric is “mAP@0.5”)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za4WVk63Qf8t"
      },
      "source": [
        "### If you think carefully we have quite all blocks needed!\r\n",
        "\r\n",
        "      Convolutions + new criterion\r\n",
        "\r\n",
        "      What else?\r\n",
        "      - New Data Structure\r\n",
        "      - New Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcyruL_M8QNM"
      },
      "source": [
        "## Introduction CNN for Obj Detection\n",
        "\n",
        "- <font color=BE3315>**R-CNN idea** </font> \n",
        "- <font color=C24024>**Variants like Fast, Faster and Mask** </font> \n",
        "- <font color=E15234>**SSD** </font> \n",
        "\n",
        "- <font color=E35F2A>**YOLO** </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfERpSWN8QNM"
      },
      "source": [
        "## R-CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rv9DM10RiX3"
      },
      "source": [
        "R-CNN or “Region-based Convolutional Neural Networks”. \r\n",
        "\r\n",
        "Two steps :\r\n",
        "- using selective search and it identifies a manageable number of bounding-box object region candidates (“region of interest” or “**RoI**”).\r\n",
        "- And then it extracts CNN features from each region independently for classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6vxCbGxSA6c"
      },
      "source": [
        "![](https://lilianweng.github.io/lil-log/assets/images/RCNN.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnXMlUpgUaHn"
      },
      "source": [
        "## WORKFLOW\r\n",
        "\r\n",
        "1) Pre-train a CNN network on image classification tasks. Like **VGG or ResNet**. *Pretrained models are welcome!* . The classification task involves N classes\r\n",
        "\r\n",
        "2) Propose category-independent regions of interest by **selective search**\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l93iQjtjVZzw"
      },
      "source": [
        "-----------------------------------------------------------------\r\n",
        "\r\n",
        "*What is selective search??*\r\n",
        "\r\n",
        "<img src=\"https://lilianweng.github.io/lil-log/assets/images/selective-search-algorithm.png\" width=\"400\">\r\n",
        "\r\n",
        "*Main idea*\r\n",
        "\r\n",
        "- First the similarities between all neighbouring regions are calculated.\r\n",
        "- The two most similar regions are grouped together, and new similarities are calculated between the resulting region and its neighbours.\r\n",
        "\r\n",
        "-----------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1G4Ake_SA9D"
      },
      "source": [
        "3) Region candidates are warped to have a fixed size as required by CNN. (**Why?**)\r\n",
        "\r\n",
        "4) Fine tuning on warped regions with number of classes + 1 (background)\r\n",
        "\r\n",
        "5) Given every image region, one forward propagation through the CNN generates a feature vector. This feature vector is then consumed by a **binary** SVM (can you say me why SVM??) trained for each class **independently**. \r\n",
        "\r\n",
        "- Proposed regions are the ones with IoU (intersection over union) higher than a given threshold\r\n",
        "\r\n",
        "6) To reduce the localization errors, a regression model is trained to correct the predicted detection window on bounding box correction offset using CNN features \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF9PyZelY-a1"
      },
      "source": [
        "*We will not define into details this step. If you're interested this is called Bounding Box Regression* **BUT** the main idea is the following :\r\n",
        "\r\n",
        "<img src=\"https://media.springernature.com/original/springer-static/image/chp%3A10.1007%2F978-3-030-20876-9_24/MediaObjects/484523_1_En_24_Fig1_HTML.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrkWOe7FRicu"
      },
      "source": [
        "## Fast R-CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SotbbIDQRkM3"
      },
      "source": [
        "<img src=\"https://lilianweng.github.io/lil-log/assets/images/fast-RCNN.png\" width=\"500\">\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz-yqmbKRkPM"
      },
      "source": [
        "*Computation Sharing for speed-up R-CNN*\r\n",
        "\r\n",
        "- Instead of extracting CNN feature vectors independently for each region proposal, this model aggregates them into one CNN forward pass over the entire image (This is obvious if you think about the logic behind MLP to CNN) and the region proposals share this feature matrix. \r\n",
        "\r\n",
        "- The same feature matrix is branched out to be used for learning the object classifier and the bounding-box regressor\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twGGJhExa6GI"
      },
      "source": [
        "## WORKFLOW\r\n",
        "\r\n",
        "1-2 like previous one\r\n",
        "\r\n",
        "3) Modify the CNN architecture :\r\n",
        "- Replace the last max pooling layer of the pre-trained CNN with a **RoI pooling layer**. The RoI pooling layer outputs fixed-length feature vectors of region proposals. Sharing the CNN computation makes a lot of sense, as many region proposals of the same images are highly overlapped.\r\n",
        "\r\n",
        "- Replace the last fully connected layer and the last softmax layer (K classes) with a fully connected layer and softmax over K + 1 classes. (*as before*)\r\n",
        "\r\n",
        "    - We have a different Loss Function that will optimize classification + localization\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58WzMb3DbYQU"
      },
      "source": [
        "----------------------------------------------------\r\n",
        "*What is a ROI pooling?*\r\n",
        "\r\n",
        "It is a type of max pooling to convert features in the projected region of the image of any size, h x w, into a small fixed window, H x W. The input region is divided into H x W grids, approximately every subwindow of size h/H x w/W. Then apply max-pooling in each grid.\r\n",
        "\r\n",
        "----------------------------------------------------\r\n",
        "$$$$\r\n",
        "\r\n",
        "4-5) like previous model : softmax on K+1 classes and BB regression\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ3e0AxrgYGq"
      },
      "source": [
        "**Anyhow**\r\n",
        "\r\n",
        "Fast R-CNN is much faster in both training and testing time. However, the improvement is not dramatic because the region proposals are generated separately by another model and that is very expensive\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao7KP7BDRkU6"
      },
      "source": [
        "## Faster R-CNN\r\n",
        "\r\n",
        "<img src=\"https://lilianweng.github.io/lil-log/assets/images/faster-RCNN.png\" width=\"500\">\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "*As said before :*\r\n",
        "\r\n",
        "-  integrate the region proposal algorithm into the CNN model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2OaUkI6hfPO"
      },
      "source": [
        "## WORKFLOW\r\n",
        "\r\n",
        "1) equal to previous models\r\n",
        "2) Fine-tune the RPN (region proposal network) end-to-end for the region proposal task, which is initialized by the pre-train image classifier. Positive samples have IoU higher of a give threshold (as before)\r\n",
        "  - Slide a small n x n spatial window over the conv feature map of the entire image.\r\n",
        "  - At the center of each sliding window, we predict multiple regions of various scales and ratios simultaneously\r\n",
        "\r\n",
        "3) Train a Fast R-CNN object detection model using the proposals generated by the current RPN\r\n",
        "\r\n",
        "4) Then use the Fast R-CNN network to initialize RPN training. While keeping the shared convolutional layers, only fine-tune the RPN-specific layers. At this stage, RPN and the detection network have shared convolutional layers\r\n",
        "\r\n",
        "5) Fine-tune the unique layers of Fast R-CNN\r\n",
        "\r\n",
        "*Step 4-5 can be repeated to train RPN and Fast R-CNN alternatively if needed.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydTfDdcNRmUk"
      },
      "source": [
        "## Mask R-CNN\r\n",
        "\r\n",
        "\r\n",
        "- Not a faster model\r\n",
        "- First attempt of CNN to Image Segmentation\r\n",
        "  - Extend Faster R-CNN to **pixel-level**\r\n",
        "\r\n",
        "<img src=\"https://lilianweng.github.io/lil-log/assets/images/mask-rcnn.png\" width=\"500\">\r\n",
        "\r\n",
        "\r\n",
        "Because pixel-level segmentation requires much more fine-grained alignment than bounding boxes, mask R-CNN improves the **RoI pooling layer**\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jwPHklBSHxM"
      },
      "source": [
        "## R-CNN Family :\n",
        "\n",
        "![](https://lilianweng.github.io/lil-log/assets/images/rcnn-family-summary.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nthOm704BGqV"
      },
      "source": [
        "## From two-stage to one-stage detector\r\n",
        "\r\n",
        "*Until now we had two different stages :*\r\n",
        "\r\n",
        "      - Define RoI\r\n",
        "      - classifier of RoI\r\n",
        "\r\n",
        "*Let's consider another approach*\r\n",
        "\r\n",
        "*Why?* \r\n",
        "\r\n",
        "Consider that we are interested in a Real-Time object detector. Our inference must be super fast!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNuicxHUTwIi"
      },
      "source": [
        "## YOLO \r\n",
        "      [You Only Look Once]\r\n",
        "\r\n",
        "<img src=\"https://lilianweng.github.io/lil-log/assets/images/yolo-network-architecture.png\" width=\"500\">\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xuxh902YTv_s"
      },
      "source": [
        "### WORKFLOW\r\n",
        "\r\n",
        "1) Pre-train a CNN network on image classification task.\r\n",
        "\r\n",
        "2) Split an image into S×S cells. If an object’s center falls into a cell, that cell is “responsible” for detecting the existence of that object. Each cell predicts : \r\n",
        "- the location of B bounding boxes \r\n",
        "\r\n",
        "      (4d tuple)\r\n",
        "      --->  (center x-coord, center y-coord, width, height)\r\n",
        "\r\n",
        "- a confidence score\r\n",
        "\r\n",
        "      P(containing an object) x IoU(pred, truth)\r\n",
        "\r\n",
        "\r\n",
        "-  a probability of object class conditioned on the existence of an object in the bounding box.\r\n",
        "\r\n",
        "        If box has confidence score :\r\n",
        "        ---> array of probability for each class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yzH70S1TutA"
      },
      "source": [
        "## SSD\r\n",
        "\r\n",
        "      Single Shot MultiBox Detector\r\n",
        "\r\n",
        "<img src=\"https://lilianweng.github.io/lil-log/assets/images/SSD-architecture.png\" width=\"500\">\r\n",
        "\r\n",
        "\r\n",
        "Main topics abot SSD :\r\n",
        "\r\n",
        "- Massive use of pre-trained models\r\n",
        "  - Pyramidal Features (do you remember the differences between ResNet and VGG?)\r\n",
        "\r\n",
        "- Unlike YOLO, SSD does not split the image into grids of arbitrary size but predicts offset of predefined **anchor boxes**\r\n",
        "\r\n",
        " "
      ]
    }
  ]
}